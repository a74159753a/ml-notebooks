{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq With Attention Using Keras\n",
    "This notebook has two parts. The first part implements a Seq2Seq networking using pure Keras. The second part adds attention to that network.\n",
    "\n",
    "The notebook uses the same dataset as the official Tensorflow seq2seq tutorial:\n",
    "https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb\n",
    "\n",
    "I put this implementation together because I'm not a huge fan of the eager execution mode in that official tutorial: I find it harder to read & follow and it is not very efficient at using computational resources (e.g. this implementation is faster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing all the things we'll need.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, CuDNNLSTM, Flatten, TimeDistributed, Dropout, LSTMCell, RNN\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# This enables the Jupyter backend on some matplotlib installations.\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "# Turn off interactive plots. iplt doesn't work well with Jupyter.\n",
    "plt.ioff()\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling and Pre-processing the Data\n",
    "The first part of this notebook is pretty much a copy and paste from the aforementioned Tensorflow tutorial: we are using the exact same data as they do.\n",
    "\n",
    "The only difference is that I've changed the code to translate from English to Spanish, not the other way around. I speak English natively and speak a little Spanish - so this feels more appropriate to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        self.create_index()\n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "        self.vocab = sorted(self.vocab)\n",
    "        self.word2idx[\"<pad>\"] = 0\n",
    "        self.idx2word[0] = \"<pad>\"\n",
    "        for i,word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = i + 1\n",
    "            self.idx2word[i+1] = word\n",
    "\n",
    "# Download the file\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://download.tensorflow.org/data/spa-eng.zip', \n",
    "    extract=True)\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    w = \"<start> \" + w + \" <end>\"\n",
    "    return w\n",
    "\n",
    "def max_length(t):\n",
    "    return max(len(i) for i in t)\n",
    "\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = open(path, encoding=\"UTF-8\").read().strip().split(\"\\n\")\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split(\"\\t\")] for l in lines[:num_examples]]\n",
    "    return word_pairs\n",
    "\n",
    "def load_dataset(path, num_examples):\n",
    "    pairs = create_dataset(path, num_examples)\n",
    "    out_lang = LanguageIndex(sp for en, sp in pairs)\n",
    "    in_lang = LanguageIndex(en for en, sp in pairs)\n",
    "    input_data = [[in_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n",
    "    output_data = [[out_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n",
    "\n",
    "    max_length_in, max_length_out = max_length(input_data), max_length(output_data)\n",
    "    input_data = tf.keras.preprocessing.sequence.pad_sequences(input_data, maxlen=max_length_in, padding=\"post\")\n",
    "    output_data = tf.keras.preprocessing.sequence.pad_sequences(output_data, maxlen=max_length_out, padding=\"post\")\n",
    "    return input_data, output_data, in_lang, out_lang, max_length_in, max_length_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One difference with our implementation is that we will need to provide two tensors for the target language. One is the \"teacher tensor\", which forces the decoder to follow a correct output stream. The other is the true \"target tensor\", which defines what the decoder should output given the teacher tensor. \n",
    "\n",
    "The only difference between the two is that the target tensor is just the teacher tensor shifted left by one word (and missing the \"<START>\" word).\n",
    "    \n",
    "To allow these two tensors to work in tandem, we will create two separate numpy arrays, even though the data is pretty much identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_examples = 118000 # Full example set.\n",
    "num_examples = 30000 # Partial set for faster training\n",
    "input_data, teacher_data, input_lang, target_lang, len_input, len_target = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "\n",
    "target_data = [[teacher_data[n][i+1] for i in range(len(teacher_data[n])-1)] for n in range(len(teacher_data))]\n",
    "target_data = tf.keras.preprocessing.sequence.pad_sequences(target_data, maxlen=len_target, padding=\"post\")\n",
    "target_data = target_data.reshape((target_data.shape[0], target_data.shape[1], 1))\n",
    "\n",
    "# Shuffle all of the data in unison. This training set has the longest (e.g. most complicated) data at the end,\n",
    "# so a simple Keras validation split will be problematic if not shuffled.\n",
    "p = np.random.permutation(len(input_data))\n",
    "input_data = input_data[p]\n",
    "teacher_data = teacher_data[p]\n",
    "target_data = target_data[p]\n",
    "\n",
    "BUFFER_SIZE = len(input_data)\n",
    "BATCH_SIZE = 64\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_in_size = len(input_lang.word2idx)\n",
    "vocab_out_size = len(target_lang.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Seq2Seq Model\n",
    "Now, let's create a Keras model which will be used to implement the encoder and decoder. This will be done with the Keras Functional API.\n",
    "\n",
    "There are two major components: the encoder and the decoder. We create this first model to train both concurrently by hooking them together. Later on we will use the trained weights from this model to create two separate models for inference.\n",
    "\n",
    "Note that the Tensorflow tutorial uses a GRU while I will be using an LSTM. There isn't a huge difference between the two, other than that I believe the LSTM does a better job at this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Encoder layers first.\n",
    "encoder_inputs = Input(shape=(len_input,))\n",
    "encoder_emb = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)\n",
    "encoder_lstm = CuDNNLSTM(units=units, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_emb(encoder_inputs))\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Now create the Decoder layers.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_emb = Embedding(input_dim=vocab_out_size, output_dim=embedding_dim)\n",
    "decoder_lstm = CuDNNLSTM(units=units, return_sequences=True, return_state=True)\n",
    "decoder_lstm_out, _, _ = decoder_lstm(decoder_emb(decoder_inputs), initial_state=encoder_states)\n",
    "# Two dense layers added to this model to improve inference capabilities.\n",
    "decoder_d1 = Dense(units, activation=\"relu\")\n",
    "decoder_d2 = Dense(vocab_out_size, activation=\"softmax\")\n",
    "# Drop-out is added in the dense layers to help mitigate overfitting in this part of the model. Astute developers\n",
    "# may want to add the same mechanism inside the LSTMs.\n",
    "decoder_out = decoder_d2(Dropout(rate=.4)(decoder_d1(Dropout(rate=.4)(decoder_lstm_out))))\n",
    "\n",
    "# Finally, create a training model which combines the encoder and the decoder.\n",
    "# Note that this model has three inputs:\n",
    "#  encoder_inputs=[batch,encoded_words] from input language (English)\n",
    "#  decoder_inputs=[batch,encoded_words] from output language (Spanish). This is the \"teacher tensor\".\n",
    "#  decoder_out=[batch,encoded_words] from output language (Spanish). This is the \"target tensor\".\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_out)\n",
    "# We'll use sparse_categorical_crossentropy so we don't have to expand decoder_out into a massive one-hot array.\n",
    "#  Adam is used because it's, well, the best.\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), loss=\"sparse_categorical_crossentropy\", metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell trains the model and plots the training and verification losses. It will take a **long** time, especially if you use the full data set. If you run into memory errors, reduce the data set size (less words in the language), the batch size, or the dimensionality of the model.\n",
    "\n",
    "When training this model, I saw modest improvements all the way up to Epoch 30 when using the full dataset. This took around 2 hours on an RTX 2080Ti. I'd recommend using a smaller dataset and less Epoch's if you are just playing around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, we use 20% of our data for validation.\n",
    "epochs = 10\n",
    "history = model.fit([input_data, teacher_data], target_data,\n",
    "                 batch_size=BATCH_SIZE,\n",
    "                 epochs=epochs,\n",
    "                 validation_split=0.2)\n",
    "\n",
    "# Plot the results of the training.\n",
    "plt.plot(history.history['sparse_categorical_accuracy'], label=\"Training loss\")\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'], label=\"Validation loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the full dataset, you should get an accuracy of up to 95% on the training data with a validation accuracy just below 89%. The validation accuracy starts to plateau around epoch 15, suggesting this model isn't really inferring anymore and is just overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "The trick to our Keras seq2seq model is that Inference will require two separate models from training. This is because during training, we used a \"teacher tensor\" to force the model to continuously make predictions assuming it had gotten everything before right. Now we want it to make those predictions all by itself.\n",
    "\n",
    "To accomplish this, we need to break up the encoder and decoder mechanisms. We will then run the entire input sequence through the encoder, then form the output by executing the decoder one step at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the encoder model from the tensors we previously declared.\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Generate a new set of tensors for our new inference decoder. Note that we are using new tensors, \n",
    "# this does not preclude using the same underlying layers that we trained on. (e.g. weights/biases).\n",
    "inf_decoder_inputs = Input(shape=(None,), name=\"inf_decoder_inputs\")\n",
    "# We'll need to force feed the two state variables into the decoder each step.\n",
    "state_input_h = Input(shape=(units,), name=\"state_input_h\")\n",
    "state_input_c = Input(shape=(units,), name=\"state_input_c\")\n",
    "decoder_res, decoder_h, decoder_c = decoder_lstm(\n",
    "    decoder_emb(inf_decoder_inputs), \n",
    "    initial_state=[state_input_h, state_input_c])\n",
    "inf_decoder_out = decoder_d2(decoder_d1(decoder_res))\n",
    "inf_model = Model(inputs=[inf_decoder_inputs, state_input_h, state_input_c], \n",
    "                  outputs=[inf_decoder_out, decoder_h, decoder_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our models, let's define some functions that will assist us in actually translating a given string of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the given sentence (just a string) into a vector of word IDs\n",
    "# using the language specified. This can be used for either the input (English)\n",
    "# or target (Spanish) languages.\n",
    "# Output is 1-D: [timesteps/words]\n",
    "def sentence_to_vector(sentence, lang):\n",
    "    pre = preprocess_sentence(sentence)\n",
    "    vec = np.zeros(len_input)\n",
    "    sentence_list = [lang.word2idx[s] for s in pre.split(' ')]\n",
    "    for i,w in enumerate(sentence_list):\n",
    "        vec[i] = w\n",
    "    return vec\n",
    "\n",
    "# Given an input string, an encoder model (infenc_model) and a decoder model (infmodel),\n",
    "# return a translated string.\n",
    "def translate(input_sentence, infenc_model, infmodel, attention=False):\n",
    "    sv = sentence_to_vector(input_sentence, input_lang)\n",
    "    # Reshape so we can use the encoder model. New shape=[samples,sequence length]\n",
    "    sv = sv.reshape(1,len(sv))\n",
    "    [emb_out, sh, sc] = infenc_model.predict(x=sv)\n",
    "    \n",
    "    i = 0\n",
    "    start_vec = target_lang.word2idx[\"<start>\"]\n",
    "    stop_vec = target_lang.word2idx[\"<end>\"]\n",
    "    # We will continuously feed cur_vec as an input into the decoder to produce the next word,\n",
    "    # which will be assigned to cur_vec. Start it with \"<start>\".\n",
    "    cur_vec = np.zeros((1,1))\n",
    "    cur_vec[0,0] = start_vec\n",
    "    cur_word = \"<start>\"\n",
    "    output_sentence = \"\"\n",
    "    # Start doing the feeding. Terminate when the model predicts an \"<end>\" or we reach the end\n",
    "    # of the max target language sentence length.\n",
    "    while cur_word != \"<end>\" and i < (len_target-1):\n",
    "        i += 1\n",
    "        if cur_word != \"<start>\":\n",
    "            output_sentence = output_sentence + \" \" + cur_word\n",
    "        x_in = [cur_vec, sh, sc]\n",
    "        # This will allow us to accomodate attention models, which we will talk about later.\n",
    "        if attention:\n",
    "            x_in += [emb_out]\n",
    "        [nvec, sh, sc] = infmodel.predict(x=x_in)\n",
    "        # The output of the model is a massive softmax vector with one spot for every possible word. Convert\n",
    "        # it to a word ID using argmax().\n",
    "        cur_vec[0,0] = np.argmax(nvec[0,0])\n",
    "        cur_word = target_lang.idx2word[np.argmax(nvec[0,0])]\n",
    "    return output_sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test out the model! Feel free to modify as you see fit. Note that only words\n",
    "# that we've trained the model on will be available, otherwise you'll get an error.\n",
    "print(translate(\"I love you\", encoder_model, inf_model))\n",
    "print(translate(\"I am hungry\", encoder_model, inf_model))\n",
    "print(translate(\"I know what you said.\", encoder_model, inf_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Attention to Seq2Seq\n",
    "If you were to dig deeper into the performance of the model above, I suspect you would find one of 3 major sources of errors:\n",
    "- Words that only appear once or twice in the training data get mis-translated. (Not enough data)\n",
    "- Words with locality difference between input and output sentences get mis-translated. E.g. in English a word appears at the start of the sentence while in Spanish it appears at the end.\n",
    "- The dataset contains many sentences with different translations. These will always incur errors in our model.\n",
    "\n",
    "While it will be hard to fix the first problem without finding more data, we can do something about the second problem by creating a better model.\n",
    "\n",
    "Attention is a concept which was designed to help fix this temporal limitation. In the below code, I implement attention in Keras in an object oriented way: I do not re-invent LSTM or Dense math, I instead inject attention directly into the \"LSTMCell\" class by deriving it and re-defining its core methods. I think this makes the implementation comparatively easy to read, I hope you agree.\n",
    "\n",
    "One additional note before we dig into the code. You'll note that above we use the \"CuDNNLSTM\" class, while below we use the \"LSTM\" class. This comes at a substantial performance penalty during training: between 2-4x. This is necessary because \"CuDNNLSTM\" is nothing but a wrapper around a CuDNN library call which performs training on an LSTM across an entire input sequence. The API does not offer a mechanism for manipulating the data being fed into the LSTM at each timestep, which we will need to do to implement atttention. This isn't to say that using the CuDNN optimizations with attention is impossible, it's just not something I dug into while putting this together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'lstm_with_attention/transpose_1:0' shape=(?, 23, ?) dtype=float32>,\n",
       " <tf.Tensor 'lstm_with_attention/while/Exit_3:0' shape=(?, 1024) dtype=float32>,\n",
       " <tf.Tensor 'lstm_with_attention/while/Exit_4:0' shape=(?, 1024) dtype=float32>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RNN \"Cell\" classes in Keras perform the actual data transformations at each timestep. Therefore, in order\n",
    "# to add attention to LSTM, we need to make a custom subclass of LSTMCell.\n",
    "class AttentionLSTMCell(LSTMCell):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.attentionMode = False\n",
    "        super(AttentionLSTMCell, self).__init__(**kwargs)\n",
    "    \n",
    "    # Build is called to initialize the variables that our cell will use. We will let other Keras\n",
    "    # classes (e.g. \"Dense\") actually initialize these variables.\n",
    "    @tf_utils.shape_type_conversion\n",
    "    def build(self, input_shape):        \n",
    "        # Converts the input sequence into a sequence which can be matched up to the internal\n",
    "        # hidden state.\n",
    "        self.dense_constant = TimeDistributed(Dense(self.units, name=\"AttLstmInternal_DenseConstant\"))\n",
    "        \n",
    "        # Transforms the internal hidden state into something that can be used by the attention\n",
    "        # mechanism.\n",
    "        self.dense_state = Dense(self.units, name=\"AttLstmInternal_DenseState\")\n",
    "        \n",
    "        # Transforms the combined hidden state and converted input sequence into a vector of\n",
    "        # probabilities for attention.\n",
    "        self.dense_transform = Dense(1, name=\"AttLstmInternal_DenseTransform\")\n",
    "        \n",
    "        # We will augment the input into LSTMCell by concatenating the context vector. Modify\n",
    "        # input_shape to reflect this.\n",
    "        batch, input_dim = input_shape[0]\n",
    "        batch, timesteps, context_size = input_shape[-1]\n",
    "        lstm_input = (batch, input_dim + context_size)\n",
    "        \n",
    "        # The LSTMCell superclass expects no constant input, so strip that out.\n",
    "        return super(AttentionLSTMCell, self).build(lstm_input)\n",
    "    \n",
    "    # This must be called before call(). The \"input sequence\" is the output from the \n",
    "    # encoder. This function will do some pre-processing on that sequence which will\n",
    "    # then be used in subsequent calls.\n",
    "    def setInputSequence(self, input_seq):\n",
    "        self.input_seq = input_seq\n",
    "        self.input_seq_shaped = self.dense_constant(input_seq)\n",
    "        self.timesteps = tf.shape(self.input_seq)[-2]\n",
    "    \n",
    "    # This is a utility method to adjust the output of this cell. When attention mode is\n",
    "    # turned on, the cell outputs attention probability vectors across the input sequence.\n",
    "    def setAttentionMode(self, mode_on=False):\n",
    "        self.attentionMode = mode_on\n",
    "    \n",
    "    # This method sets up the computational graph for the cell. It implements the actual logic\n",
    "    # that the model follows.\n",
    "    def call(self, inputs, states, constants):\n",
    "        # Separate the state list into the two discrete state vectors.\n",
    "        # ytm is the \"memory state\", stm is the \"carry state\".\n",
    "        ytm, stm = states\n",
    "        # We will use the \"carry state\" to guide the attention mechanism. Repeat it across all\n",
    "        # input timesteps to perform some calculations on it.\n",
    "        stm_repeated = K.repeat(self.dense_state(stm), self.timesteps)\n",
    "        # Now apply our \"dense_transform\" operation on the sum of our transformed \"carry state\" \n",
    "        # and all encoder states. This will squash the resultant sum down to a vector of size\n",
    "        # [batch,timesteps,1]\n",
    "        combined_stm_input = self.dense_transform(\n",
    "            keras.activations.tanh(stm_repeated + self.input_seq_shaped))\n",
    "        # Performing a softmax generates a log probability for each encoder output to receive attention.\n",
    "        score_vector = keras.activations.softmax(combined_stm_input, 1)\n",
    "        # In this implementation, we grant \"partial attention\" to each encoder output based on \n",
    "        # it's log probability accumulated above. Other options would be to only give attention\n",
    "        # to the highest probability encoder output or some similar set.\n",
    "        context_vector = K.sum(score_vector * self.input_seq, 1)\n",
    "        \n",
    "        # Finally, mutate the input vector. It will now contain the traditional inputs (like the seq2seq\n",
    "        # we trained above) in addition to the attention context vector we calculated earlier in this method.\n",
    "        inputs = K.concatenate([inputs, context_vector])\n",
    "        \n",
    "        # Call into the super-class to invoke the LSTM math.\n",
    "        res = super(AttentionLSTMCell, self).call(inputs=inputs, states=states)\n",
    "        \n",
    "        # This if statement switches the return value of this method if \"attentionMode\" is turned on.\n",
    "        if(self.attentionMode):\n",
    "            return (K.reshape(score_vector, (-1, self.timesteps)), res[1])\n",
    "        else:\n",
    "            return res\n",
    "\n",
    "# Custom implementation of the Keras LSTM that adds an attention mechanism.\n",
    "# This is implemented by taking an additional input (using the \"constants\" of the\n",
    "# RNN class) into the LSTM: The encoder output vectors across the entire input sequence.\n",
    "class LSTMWithAttention(RNN):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        cell = AttentionLSTMCell(units=units)\n",
    "        self.units = units\n",
    "        super(LSTMWithAttention, self).__init__(cell, **kwargs)\n",
    "        \n",
    "    @tf_utils.shape_type_conversion\n",
    "    def build(self, input_shape):\n",
    "        self.input_dim = input_shape[0][-1]\n",
    "        self.timesteps = input_shape[0][-2]\n",
    "        return super(LSTMWithAttention, self).build(input_shape) \n",
    "    \n",
    "    # This call is invoked with the entire time sequence. The RNN sub-class is responsible\n",
    "    # for breaking this up into calls into the cell for each step.\n",
    "    # The \"constants\" variable is the key to our implementation. It was specifically added\n",
    "    # to Keras to accomodate the \"attention\" mechanism we are implementing.\n",
    "    def call(self, x, constants, **kwargs):\n",
    "        if isinstance(x, list):\n",
    "            self.x_initial = x[0]\n",
    "        else:\n",
    "            self.x_initial = x\n",
    "        \n",
    "        # The only difference in the LSTM computational graph really comes from the custom\n",
    "        # LSTM Cell that we utilize.\n",
    "        self.cell._dropout_mask = None\n",
    "        self.cell._recurrent_dropout_mask = None\n",
    "        self.cell.setInputSequence(constants[0])\n",
    "        return super(LSTMWithAttention, self).call(inputs=x, constants=constants, **kwargs)\n",
    "\n",
    "# Below is test code to validate that this LSTM class and the associated cell create a\n",
    "# valid computational graph.\n",
    "test = LSTMWithAttention(units=units, return_sequences=True, return_state=True)\n",
    "test.cell.setAttentionMode(True)\n",
    "attenc_inputs2 = Input(shape=(len_input,))\n",
    "attenc_emb2 = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)\n",
    "test(inputs=attenc_emb2(attenc_inputs2), constants=attenc_emb2(attenc_inputs2), initial_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a custom LSTM class which implements attention, we can use very similar code to what we did above for basic seq2seq to train a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Re-create an entirely new model and set of layers for the attention model\n",
    "\n",
    "# Encoder Layers\n",
    "attenc_inputs = Input(shape=(len_input,), name=\"attenc_inputs\")\n",
    "attenc_emb = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)\n",
    "attenc_lstm = CuDNNLSTM(units=units, return_sequences=True, return_state=True)\n",
    "attenc_outputs, attstate_h, attstate_c = attenc_lstm(attenc_emb(attenc_inputs))\n",
    "attenc_states = [attstate_h, attstate_c]\n",
    "\n",
    "attdec_inputs = Input(shape=(None,))\n",
    "attdec_emb = Embedding(input_dim=vocab_out_size, output_dim=embedding_dim)\n",
    "attdec_lstm = LSTMWithAttention(units=units, return_sequences=True, return_state=True)\n",
    "# Note that the only real difference here is that we are feeding attenc_outputs to the decoder now.\n",
    "# Nice and clean!\n",
    "attdec_lstm_out, _, _ = attdec_lstm(inputs=attdec_emb(attdec_inputs), \n",
    "                                    constants=attenc_outputs, \n",
    "                                    initial_state=attenc_states)\n",
    "attdec_d1 = Dense(units, activation=\"relu\")\n",
    "attdec_d2 = Dense(vocab_out_size, activation=\"softmax\")\n",
    "attdec_out = attdec_d2(Dropout(rate=.4)(attdec_d1(Dropout(rate=.4)(attdec_lstm_out))))\n",
    "\n",
    "attmodel = Model([attenc_inputs, attdec_inputs], attdec_out)\n",
    "attmodel.compile(optimizer=tf.train.AdamOptimizer(), loss=\"sparse_categorical_crossentropy\", metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expect for training the attention model to take considerably longer than the non-attention model. I suspect the major reason for this is that CuDNNLSTM cannot be used. The additional dense computations for each timestep also no doubt adds to the computational complexity of this problem.\n",
    "\n",
    "I was seeing training times of 6 hours for the full dataset on my RTX 2080Ti. Validation accuracy plateaued around epoch 20 with a strong indication of overfitting occurring past that. The final accuracy was 95.7% on the training set and slightly above 90% on the validation set.\n",
    "\n",
    "\"All this for a ~1.3% gain in accuracy?\" you might say. I would argue that this dataset does not benefit as much from attention as others might. It does not contain many long sentences, which is where attention really shines. It also seems, from testing, that the attention model performs particularly well on longer sentences and particularly poorly on small sentences - though this may be a result of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 94400 samples, validate on 23600 samples\n",
      "Epoch 1/20\n",
      "94400/94400 [==============================] - 234s 2ms/step - loss: 1.6899 - sparse_categorical_accuracy: 0.7488 - val_loss: 1.4129 - val_sparse_categorical_accuracy: 0.7784\n",
      "Epoch 2/20\n",
      "63488/94400 [===================>..........] - ETA: 1:08 - loss: 1.3314 - sparse_categorical_accuracy: 0.7879"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "atthist = attmodel.fit([input_data, teacher_data], target_data,\n",
    "                 batch_size=BATCH_SIZE,\n",
    "                 epochs=epochs,\n",
    "                 validation_split=0.2)\n",
    "\n",
    "# Plot the results of the training.\n",
    "plt.plot(atthist.history['sparse_categorical_accuracy'], label=\"Training loss\")\n",
    "plt.plot(atthist.history['val_sparse_categorical_accuracy'], label=\"Validation loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference With Attention\n",
    "Now that we've got an attention model, let's test it, similar to above. The inference models don't change much from our seq2seq implementation, again with the exception of feeding in the encoder outputs to the decoder (which we already did in train() above).\n",
    "\n",
    "One thing I noticed about this attention model is that it actually seems to perform worse on completely novel data (e.g. phrases that aren't at all related to the ones in the training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAttentionInference(attention_mode=False):\n",
    "    # Create an inference model using the layers already trained above.\n",
    "    attencoder_model = Model(attenc_inputs, [attenc_outputs, attstate_h, attstate_c])\n",
    "    state_input_h = Input(shape=(units,), name=\"state_input_h\")\n",
    "    state_input_c = Input(shape=(units,), name=\"state_input_c\")\n",
    "    attenc_seq_out = Input(shape=attenc_outputs.get_shape()[1:], name=\"attenc_seq_out\")\n",
    "    inf_attdec_inputs = Input(shape=(None,), name=\"inf_attdec_inputs\")\n",
    "    attdec_lstm.cell.setAttentionMode(attention_mode)\n",
    "    attdec_res, attdec_h, attdec_c = attdec_lstm(attdec_emb(inf_attdec_inputs), \n",
    "                                                 initial_state=[state_input_h, state_input_c], \n",
    "                                                 constants=attenc_seq_out)\n",
    "    attinf_model = None\n",
    "    if not attention_mode:\n",
    "        inf_attdec_out = attdec_d2(attdec_d1(attdec_res))\n",
    "        attinf_model = Model(inputs=[inf_attdec_inputs, state_input_h, state_input_c, attenc_seq_out], \n",
    "                             outputs=[inf_attdec_out, attdec_h, attdec_c])\n",
    "    else:\n",
    "        attinf_model = Model(inputs=[inf_attdec_inputs, state_input_h, state_input_c, attenc_seq_out], \n",
    "                             outputs=[attdec_res, attdec_h, attdec_c])\n",
    "    return attencoder_model, attinf_model\n",
    "\n",
    "attencoder_model, attinf_model = createAttentionInference()\n",
    "print(translate(\"I love you\", attencoder_model, attinf_model, True))\n",
    "print(translate(\"I am hungry\", attencoder_model, attinf_model, True))\n",
    "print(translate(\"What is your name.\", attencoder_model, attinf_model, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other neat thing we can do with attention is investigate what the model is paying attention to in the encoder inputs when it is performing translations. We added this functionality to our LSTMAttentionCell class already, we just need to turn it on. \n",
    "\n",
    "Note that when this feature is turned on, the decoder no longer outputs word IDs. This means we'll need to revert back to using our teacher data to guide the decoder through an output phrase while we track where the model is paying attention.\n",
    "\n",
    "The below cell generates a table that is colored according to attention. Hotter, yellow colors correspond to higher attention, while darker blues correspond to less attention.\n",
    "\n",
    "If you bother to train the full data set, play around with this a bit. I feel compelled to point out how remarkable this learned behavior is. If you use this same function with untrained weights, the matrix below is randomly distributed with great uniformity. The training process creates all of the variety that you see, and simply because we constrained our model in a particular way. This emergence of meaning and overall semantic understanding from raw data is what makes machine learning so cool to me. It truly is a thing of beauty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_attention(input_sentence, output_sentence, infenc_model, infmodel):\n",
    "    sv = sentence_to_vector(input_sentence, input_lang)\n",
    "    # Shape=samples,sequence length\n",
    "    sv = sv.reshape(1,len(sv))\n",
    "    [emb_out, sh, sc] = infenc_model.predict(x=sv)\n",
    "    \n",
    "    outvec = sentence_to_vector(output_sentence, target_lang)\n",
    "    i = 0\n",
    "    cur_vec = np.zeros((1,1))\n",
    "    cur_vec[0,0] = outvec[0]\n",
    "    cur_word = \"<start>\"\n",
    "    output_attention = []\n",
    "    while i < (len(outvec)-1):\n",
    "        i += 1\n",
    "        x_in = [cur_vec, sh, sc, emb_out]\n",
    "        [nvec, sh, sc] = infmodel.predict(x=x_in)\n",
    "        output_attention += [nvec]\n",
    "        cur_vec[0,0] = outvec[i]\n",
    "    return output_attention\n",
    "\n",
    "def plotAttention(attMatrix):\n",
    "    attMatrix = np.asarray(attMatrix)\n",
    "    attMatrix = np.reshape(attMatrix, (attMatrix.shape[0], attMatrix.shape[-1]))\n",
    "    #print(attMatrix)\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attMatrix, aspect=\"auto\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "attencoder_model, attinf_model = createAttentionInference(True)\n",
    "#print(investigate_attention(\"I love me\", attencoder_model, attinf_model, True))\n",
    "#print(investigate_attention(\"I am hungry\", attencoder_model, attinf_model, True))\n",
    "plotAttention(investigate_attention(\"You can use a dictionary for this exam.\", \"Para este examen podéis usar un diccionario.\", attencoder_model, attinf_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
